{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42029268-e12c-45f9-883e-18c7300420a2",
   "metadata": {},
   "source": [
    "## LSTM Model for parsed at sentence level text (Melisa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f77f3eb-432c-410a-ae7e-a99a5c537a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(11)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\"\"\"\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "\n",
    "import tensorflow as tf\n",
    "#!pip install tensorflow_hub\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ec2530-6868-456c-94a3-4afb9af61fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from sklearn.metrics import f1_score\n",
    "#!pip install gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2d20bb-1235-4442-a928-aa8a57fbcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipynb\n",
    "#from ipynb.fs.full.preprocessing.ipynb import process_review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9341f0e7-f668-4959-b155-4a9ea74bb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"../data/train_preprocessed.json\")\n",
    "test = pd.read_json(\"../data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355491ee-756b-4bfc-954c-c24e17dba45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972168c7-bb40-4909-9e8f-131d27256522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 8)\n",
      "(10000, 8)\n",
      "(5000,)\n",
      "LAST SACRIFICE was amazing. That's all there is to say. **POST-READING EDIT:** It had a bit of the plot disorganization that SPIRIT BOUND did, and I felt like at the end, Lissa seemed a bit unhappy with her future as  she became queen.\n"
     ]
    }
   ],
   "source": [
    "# Adjust based on your vocabulary size\n",
    "#train_5 = train[:20000]\n",
    "train_5_1 = train[train[\"has_spoiler\"] == True]\n",
    "train_5_0 = train[train[\"has_spoiler\"] == False]\n",
    "train_5 = pd.concat([train_5_1[:5000], train_5_0[:10000]], axis=0)\n",
    "\n",
    "print(train_5[train_5[\"has_spoiler\"]==True].shape)\n",
    "print(train_5[train_5[\"has_spoiler\"]==False].shape)\n",
    "\n",
    "\n",
    "\n",
    "reviews_data = train_5[\"review_sentences\"]\n",
    "labels = train_5[\"has_spoiler\"]\n",
    "\n",
    "\n",
    "\n",
    "reviews_data = [' '.join([sentence for _, sentence in review]) for review in reviews_data]\n",
    "\n",
    "\n",
    "print(labels[labels == True].shape)\n",
    "\n",
    "#print(reviews_data)\n",
    "#lengths = [len(i) for i in reviews_data]\n",
    "#print(min(lengths))\n",
    "#print(reviews_data[0])\n",
    "\n",
    "\"\"\"\n",
    "X_np = np.array(reviews_data)\n",
    "\n",
    "X = tf.convert_to_tensor(X_np)\n",
    "\n",
    "y = tf.convert_to_tensor(labels)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "reviews_np = [np.array(i) for i in reviews_data]\n",
    "reviews_np = np.array(reviews_np)\n",
    "labels_np = np.array(labels)\n",
    "\"\"\"\n",
    "print(reviews_data[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643ee167-1acd-4d5a-adf1-6ef927c8302b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Train a CBOW model\\nembedding_dim = 150 # You can adjust this based on your needs\\ncbow_model = Word2Vec(sentences=sequences, vector_size=embedding_dim, window=5, sg=0, min_count=1, workers=4)\\n\\n# Save the trained model\\ncbow_model.save(\"cbow_model.bin\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 3000  # Consider only the top 3,000 words in the dataset\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(reviews_data)\n",
    "sequences = tokenizer.texts_to_sequences(reviews_data)\n",
    "\n",
    "# Padding\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Train a CBOW model\n",
    "embedding_dim = 150 # You can adjust this based on your needs\n",
    "cbow_model = Word2Vec(sentences=sequences, vector_size=embedding_dim, window=5, sg=0, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "cbow_model.save(\"cbow_model.bin\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b8c3c48-fec9-470e-b30e-23fa379bc7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Prepare the embedding matrix\\nword2vec_model = Word2Vec.load(\"cbow_model.bin\")\\nvocab_size = len(word2vec_model.wv)+1\\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\\nfor word, i in word2vec_model.wv.key_to_index.items():\\n    embedding_vector = word2vec_model.wv[word]\\n    embedding_matrix[i] = embedding_vector\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Prepare the embedding matrix\n",
    "word2vec_model = Word2Vec.load(\"cbow_model.bin\")\n",
    "vocab_size = len(word2vec_model.wv)+1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word2vec_model.wv.key_to_index.items():\n",
    "    embedding_vector = word2vec_model.wv[word]\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97d7cae-8033-4ec4-8145-81b0bf1dbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"../data/glove.6B.50d.txt\"\n",
    "def load_glove_embeddings(embeddings_path):\n",
    "    embeddings_index = {}\n",
    "    with open(embeddings_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bd5766-425d-4d93-9232-a4c4b6d4a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(max_words, len(word_index))\n",
    "embedding_dim = 50  \n",
    "embeddings_index = load_glove_embeddings(embeddings_path)\n",
    "\n",
    "        \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size: \n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32079135-0139-4b86-a6a1-68cbb301dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((padded_sequences, labels))\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "batch_size = 32\n",
    "buffer_size = 5000  # Set a buffer size for shuffling\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193fd3a1-2430-4781-b14f-a732cb2bf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "#hub_layer = hub.KerasLayer(embedding, dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff26456b-c96c-4ba3-a6ba-21f2267994b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(16))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad79daee-8b7b-4534-8fca-ac7759b5e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(hub_layer(list(reviews_data[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cafec01d-5781-4192-8f65-da938d806b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "235/235 [==============================] - 493s 2s/step - loss: 0.6327 - accuracy: 0.6649 - precision: 0.4420 - recall: 0.0198\n",
      "Epoch 2/5\n",
      "235/235 [==============================] - 492s 2s/step - loss: 0.5598 - accuracy: 0.6785 - precision: 0.5276 - recall: 0.3378\n",
      "Epoch 3/5\n",
      "235/235 [==============================] - 485s 2s/step - loss: 0.5406 - accuracy: 0.6893 - precision: 0.5444 - recall: 0.4158\n",
      "Epoch 4/5\n",
      "235/235 [==============================] - 483s 2s/step - loss: 0.5234 - accuracy: 0.7208 - precision: 0.5971 - recall: 0.4994\n",
      "Epoch 5/5\n",
      "235/235 [==============================] - 486s 2s/step - loss: 0.5162 - accuracy: 0.7299 - precision: 0.6086 - recall: 0.5318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1950a3f3190>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "#tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "#print(labels)\n",
    "\n",
    "model.fit(padded_sequences, labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e5e0c75-1257-47d9-858f-b0a869ac671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " emb (InputLayer)            [(None, 1)]               0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 1, 50)             150000    \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 1, 64)             29440     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 16)                5184      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 184,641\n",
      "Trainable params: 34,641\n",
      "Non-trainable params: 150,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ba6efa3-0738-4e54-8a12-e571bcb17142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_5_1 = test[test[\"has_spoiler\"] == True]\n",
    "test_5_0 = test[test[\"has_spoiler\"] == False]\n",
    "test_5 = pd.concat([test_5_1[:5000], test_5_0[:10000]], axis=0)\n",
    "\n",
    "\n",
    "test_data = test_5[\"review_sentences\"]\n",
    "test_labels = test_5[\"has_spoiler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da5de20e-395a-48b3-8506-cec225392cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_reviews_np = [np.array(i) for i in test_data]\\ntest_reviews_np = np.array(test_reviews_np)\\ntest_labels_np = np.array(test_labels)\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "test_data = [' '.join([sentence for _, sentence in review]) for review in test_data]\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "sequences_test = tokenizer.texts_to_sequences(test_data)\n",
    "# Padding\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_sequence_length)\n",
    "\n",
    "\"\"\"\n",
    "test_reviews_np = [np.array(i) for i in test_data]\n",
    "test_reviews_np = np.array(test_reviews_np)\n",
    "test_labels_np = np.array(test_labels)\n",
    "\"\"\"\n",
    "\n",
    "#predicted_labels = (predictions > 0.5).astype(int)\n",
    "#print(predicted_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "703d976d-fd6f-431b-81fa-36a25b429d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(padded_sequences_test, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70d4735f-ccdc-4957-8194-9bd748e4aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7279999852180481\n",
      "Test Precision: 0.597046434879303\n",
      "Test Recall: 0.5659999847412109\n",
      "Test F1: 0.5811088316522074\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test F1: {2*test_recall*test_precision / (test_recall+test_precision)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a94ac8c-10d7-443f-b421-f64ea3b72ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 122s 261ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(padded_sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ffd9c6b-ff49-4152-a464-c5f19d97b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"trained_model_lstm\", save_format=\"h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0edce70-b917-4122-92e2-f076cd963acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 126s 268ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "# RANDOM FORESTA FEEDLEMEK İÇİN\n",
    "def createLSTMModel():\n",
    "    emb_input = Input(shape=(1,), name=\"emb\")\n",
    "    embedding = layers.Embedding(input_dim=3000, output_dim=50, input_length=2048,weights=[embedding_matrix], trainable=False)(emb_input)\n",
    "    lstm1 = layers.LSTM(64, return_sequences=True)(embedding)\n",
    "    lstm2 = layers.LSTM(16)(lstm1)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(lstm2)\n",
    "    model = Model(inputs=emb_input, outputs=out)\n",
    "    return model\n",
    "\n",
    "model = createLSTMModel()\n",
    "model.load_weights(\"trained_model_lstm\") # bana attığın train edilmiş model\n",
    "pretrained_model = tf.keras.Model(\n",
    "    inputs=model.inputs,\n",
    "    outputs=model.layers[-2].output\n",
    ")\n",
    "\n",
    "X_test = padded_sequences  # train datası\n",
    "preds = pretrained_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e7cf5aa-d948-4be3-8b21-3889d00a2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_np = np.array(preds)\n",
    "np.save('lstm-preds.npy', preds_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c11634-cbc2-4805-b0ca-6ca8fe2b4bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# Get class probabilities for the training data\n",
    "probabilities_model = model.predict(X)\n",
    "probabilities_model2 = model2.predict(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#use softmax to convert prediction to probabilities\n",
    "probabilities_model = keras.activations.softmax(probabilities_model)\n",
    "probabilities_model2 = keras.activations.softmax(probabilities_model2)\n",
    "\n",
    "\n",
    "#turn class probabilities to features\n",
    "combined_features = np.column_stack((probabilities_model1, probabilities_model2))\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the random forest classifier\n",
    "rf_classifier.fit(combined_features, y_train)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7fa3d-03da-4016-942d-79762b9719ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'X_new' is your new data for prediction\n",
    "\"\"\"\n",
    "#new data\n",
    "probabilities_new_model1 = model1.predict(X_new)\n",
    "probabilities_new_model2 = model2.predict(X_new)\n",
    "\n",
    "probabilities_new_model1 = keras.activations.softmax(probabilities_new_model1)\n",
    "probabilities_new_model2 = keras.activations.softmax(probabilities_new_model2)\n",
    "\n",
    "new_data_combined_features = np.column_stack((probabilities_new_model1, probabilities_new_model2))\n",
    "\n",
    "#predictions using the stacked model\n",
    "stacked_model_predictions = rf_classifier.predict(new_data_combined_features)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305a8d8-b1a2-4f99-a39c-e89d4b58666c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
